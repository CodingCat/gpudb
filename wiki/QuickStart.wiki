#summary Quick Start Guide.
<wiki:toc max_depth="3" />

=GPUDB Start Guide=
==System Requirement==

 * A linux system with GCC (both 32-bit and 64 bit GCC are needed for 64-bit environments) and Python installed.
 * Java 1.6 
 * CUDA toolkit (4.0 or later), or AMD APP SDK or Intel OpenCL SDK.

==Source Code==
You can check out the source codes [http://code.google.com/p/gpudb/source/checkout here].

==Install Translator==
After you check out the source code, enter the source directory and execute:

   * python setup.py install

The install process only add files inside the source code directory.

If you encounter any errors during this step, make sure you have
gcc and python installed. You can also report your errors [http://code.google.com/p/ysmart/issues/entry Report]. 

==Generate Schema File==

Please refer to wiki page Schema to learn how to write schema file. An example schema file is located in test/ssb_test/ssb.schema.

==Data Generation==

Please refer to StorageFormat to get a general idea of the format of the data. You must ensure that the data are stored in the correct format before running the queries. We provide a tool that can transform the plain text data (columns must be separated by '|') into our supported storage format.

===Generate Data Loader===

Execute 
 * ./translate.py schema

to generate the loader which will transform plain text data into our supported data format.

The following shows how to transform the original SSBM data into our supported storage format. 


===SSBM Plain Data Generation===
The dbgen to generate original SSBM data is located
in test/dbgen directory. You need to generate the plain
text data. The following steps show how to generate the data with a scale factor of 1.

 * Enter test/dbgen directory, and execute "Make"
 * ./dbgen -vfF -T a -s 1 to generate data with scale factor 1


===Load SSBM Data===
The original SSBM data cannot be directly used by our engine.
The data need to be transformed into column-stored data. The following steps show how to transform the SSBM data. Assume the original SSBM data is in the /data directory.

 * Enter src/utility directory and execute "Make loader", which will generate "gpuDBLoader" to transform the original data. 

 * ./gpuDBLoader --lineorder /data/lineorder.tbl --ddate /data/date.tbl --customer /data/customer.tbl --supplier /data/supplier.tbl --part /data/part.tbl

Generally speaking, gpuDBLoader accepts arguments in the format "--table_name table_file". Each table column is stored in a disk file whose name is composed of the table name in upper case followed by the index of the column in the table. Users can specify where the data will be generated by using "--datadir dir". The data will be generated in the current directory by default.

==Data Compression==

Data can be compressed to get a better query performance. You can skip
to the next section if you don't need to compress the data.

====Run Length Encoding====

 * Sort. To get a better compression ratio, the column that is compressed using RLE scheme should be sorted first. To sort the column, users can either use the "sort" command to sort the plain text SSBM data on particular columns before transforming the data into column-stored formant, or they can enter src/utility directory and execute "make sort" to generate a file named "columnSort" that will sort a table on a particular integer column.

    Usage: ./columnSort inputPrefix outputPrefix index. inputPrefix is the name of the table to be sorted, outputPrefix is the name of the generated sorted table and index specifies the sorted column.

 * Compression. Enter src/utility and execute "make rle" to generate the file "rleCompression" to compress the column using RLE scheme.

    Usage: ./rleCompression inputColumn outputColumn. inputColumn specifies the name of the sorted column to be compressed using RLE, and outputColumn is the name of the generated compressed column.

====Dictionary Encoding====

    Enter src/utility and execute "make dict" to generate the file "dictCompression" to compress the column using dictionary encoding scheme.

    Usage: ./dictCompression inputColumn outputColumn. inputColumn specifies the name of the column to be compressed using dictionary encoding scheme, and outputColumn is the name of the generated compressed column.

==Code Generation==

Usage: *./translate.py query-file.sql schema-file.schema* 
 * query-file.sql should contain the SQL query you want to translate. Currently the suppported SQL queries are listed in test/ssb_test directory
 * schema-file.schema is the schema file that describes the data. Currently the schema file for star schema benchmark is in test/ssb_test/ssb.schema.

After executing the aforementioned command, enter src/cuda or src/opencl directory and execute "Make gpudb", which will generate an executable file
named "GPUDATABASE". Executing "GPUDATABASE --datadir dir" will run the query on GPU. The "datadir" argument specifies where the data are stored.